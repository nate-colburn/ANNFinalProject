
2024-04-25 12:28:43.354623: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2024-04-25 12:28:48.544353: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_INT32
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_FLOAT
    }
  }
}
	for Tuple type infernce function 0
	while inferring type of node 'cond_36/output/_23'
2024-04-25 12:28:49.424298: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f94327c3cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-25 12:28:49.424337: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9
2024-04-25 12:28:49.430927: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-04-25 12:28:49.446759: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1714040929.507855  409191 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.


































151/151 [==============================] - 75s 463ms/step - loss: 6.4518
































151/151 [==============================] - 66s 435ms/step - loss: 6.2334
































151/151 [==============================] - 66s 434ms/step - loss: 6.2364
































151/151 [==============================] - 66s 438ms/step - loss: 6.1987
































151/151 [==============================] - 66s 435ms/step - loss: 6.1964
































151/151 [==============================] - 67s 441ms/step - loss: 6.1803
































151/151 [==============================] - 66s 439ms/step - loss: 6.1763
































151/151 [==============================] - 66s 437ms/step - loss: 6.1688
































151/151 [==============================] - 65s 428ms/step - loss: 6.1646
































151/151 [==============================] - 66s 439ms/step - loss: 6.1607

































151/151 [==============================] - 67s 442ms/step - loss: 6.1571
































151/151 [==============================] - 66s 438ms/step - loss: 6.1547

































151/151 [==============================] - 67s 444ms/step - loss: 6.1516
































151/151 [==============================] - 67s 440ms/step - loss: 6.1500
































151/151 [==============================] - 66s 439ms/step - loss: 6.1475

































151/151 [==============================] - 66s 439ms/step - loss: 6.1463
































151/151 [==============================] - 67s 443ms/step - loss: 6.1442

































151/151 [==============================] - 67s 445ms/step - loss: 6.1433

































151/151 [==============================] - 68s 448ms/step - loss: 6.1415
































151/151 [==============================] - 66s 436ms/step - loss: 6.1407
































151/151 [==============================] - 66s 435ms/step - loss: 6.1391
































151/151 [==============================] - 66s 436ms/step - loss: 6.1385

































151/151 [==============================] - 68s 450ms/step - loss: 6.1372
































151/151 [==============================] - 66s 439ms/step - loss: 6.1366

































151/151 [==============================] - 67s 445ms/step - loss: 6.1354
































151/151 [==============================] - 66s 437ms/step - loss: 6.1350
































151/151 [==============================] - 66s 438ms/step - loss: 6.1339
































151/151 [==============================] - 66s 437ms/step - loss: 6.1335
































151/151 [==============================] - 66s 436ms/step - loss: 6.1325
































151/151 [==============================] - 66s 437ms/step - loss: 6.1321

































151/151 [==============================] - 66s 437ms/step - loss: 6.1312
































150/151 [============================>.] - ETA: 0s - loss: 6.1318
151/151 [==============================] - 67s 440ms/step - loss: 6.1309
































151/151 [==============================] - 67s 440ms/step - loss: 6.1300

































151/151 [==============================] - 66s 439ms/step - loss: 6.1298
































149/151 [============================>.] - ETA: 0s - loss: 6.1302
151/151 [==============================] - 67s 442ms/step - loss: 6.1289


































151/151 [==============================] - 69s 454ms/step - loss: 6.1287
2024-04-25 13:09:09.271384: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 129137376 bytes after encountering the first element of size 129137376 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size



































151/151 [==============================] - 72s 475ms/step - loss: 6.1279




































151/151 [==============================] - 73s 480ms/step - loss: 6.1277

































151/151 [==============================] - 66s 437ms/step - loss: 6.1269




































151/151 [==============================] - 74s 490ms/step - loss: 6.1268




































151/151 [==============================] - 74s 485ms/step - loss: 6.1260


































151/151 [==============================] - 69s 456ms/step - loss: 6.1260


































151/151 [==============================] - 68s 450ms/step - loss: 6.1252

































151/151 [==============================] - 66s 437ms/step - loss: 6.1251

































151/151 [==============================] - 68s 444ms/step - loss: 6.1244

































151/151 [==============================] - 68s 445ms/step - loss: 6.1244


































151/151 [==============================] - 68s 451ms/step - loss: 6.1236


































151/151 [==============================] - 69s 454ms/step - loss: 6.1237


































151/151 [==============================] - 70s 458ms/step - loss: 6.1230


































151/151 [==============================] - 68s 450ms/step - loss: 6.1230


































151/151 [==============================] - 71s 464ms/step - loss: 6.1223



































151/151 [==============================] - 70s 461ms/step - loss: 6.1223




































151/151 [==============================] - 73s 485ms/step - loss: 6.1216


































151/151 [==============================] - 70s 465ms/step - loss: 6.1217
2024-04-25 13:30:21.876305: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 129137376 bytes after encountering the first element of size 129137376 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size



































