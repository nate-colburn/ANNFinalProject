


















































227/227 [==============================] - 108s 448ms/step - loss: 5.9284















































227/227 [==============================] - 97s 429ms/step - loss: 5.5294















































227/227 [==============================] - 96s 421ms/step - loss: 4.7884
















































227/227 [==============================] - 98s 432ms/step - loss: 4.0403















































227/227 [==============================] - 95s 419ms/step - loss: 3.6798















































227/227 [==============================] - 97s 426ms/step - loss: 3.4445















































227/227 [==============================] - 96s 421ms/step - loss: 3.2655















































227/227 [==============================] - 96s 423ms/step - loss: 3.1211
















































227/227 [==============================] - 98s 431ms/step - loss: 3.0004
















































227/227 [==============================] - 98s 429ms/step - loss: 2.8939
















































227/227 [==============================] - 97s 428ms/step - loss: 2.8002
















































227/227 [==============================] - 98s 430ms/step - loss: 2.7152
















































227/227 [==============================] - 98s 431ms/step - loss: 2.6390















































227/227 [==============================] - 97s 425ms/step - loss: 2.5676
















































227/227 [==============================] - 97s 428ms/step - loss: 2.4976

















































227/227 [==============================] - 100s 439ms/step - loss: 2.4377
















































227/227 [==============================] - 99s 434ms/step - loss: 2.3810
















































227/227 [==============================] - 98s 433ms/step - loss: 2.3290















































227/227 [==============================] - 96s 423ms/step - loss: 2.2782













































227/227 [==============================] - 90s 398ms/step - loss: 2.2375













































227/227 [==============================] - 92s 405ms/step - loss: 2.1952














































227/227 [==============================] - 96s 421ms/step - loss: 2.1561














































227/227 [==============================] - 93s 412ms/step - loss: 2.1164















































227/227 [==============================] - 95s 417ms/step - loss: 2.0845
















































227/227 [==============================] - 97s 428ms/step - loss: 2.0527














































227/227 [==============================] - 95s 419ms/step - loss: 2.0189















































227/227 [==============================] - 95s 419ms/step - loss: 1.9880















































227/227 [==============================] - 95s 419ms/step - loss: 1.9600
  3/227 [..............................] - ETA: 1:33 - loss: 1.8901















































227/227 [==============================] - 95s 418ms/step - loss: 1.9358
2024-04-25 16:02:01.440074: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 88261536 bytes after encountering the first element of size 88261536 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size














































227/227 [==============================] - 94s 415ms/step - loss: 1.9084
/home/robert/miniconda3/envs/gpu/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(